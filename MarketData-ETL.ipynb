{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9afb9b73-9c0e-49fb-8ab8-dd3c971e25be",
   "metadata": {},
   "source": [
    "# Extraction (The Raw Input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876b7820-16fb-4973-be62-973f2ece9708",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e811bb-e7ba-4bd3-9bd1-99c4187864a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc374f-4980-42dd-becb-608d8dc2efaa",
   "metadata": {},
   "source": [
    "## Data Ingestion (Step 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26cc7d3-e14e-4383-af8e-af240be0995a",
   "metadata": {},
   "source": [
    "First, we define our parameters. We will pull over 10 years of daily data for 'SPY' (S&P 500 ETF) and 'VIX' (Cboe Volatility Index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7da3c4f-e286-4e6e-ae4c-620bc9eed231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "start_date = '2010-01-01'\n",
    "end_date = dt.date.today().strftime('%Y-%m-%d')\n",
    "tickers = ['SPY', '^VIX']\n",
    "\n",
    "# Pull data\n",
    "raw_data = yf.download(tickers, start=start_date, end=end_date, auto_adjust=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e77c77-ec4a-4a34-9cb9-f8b5a3d8f1a5",
   "metadata": {},
   "source": [
    "## Data Merge & Refinement (Step 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c2c33-cbde-457a-aad5-868adebca2e3",
   "metadata": {},
   "source": [
    "The ```yfinance``` download creates a multi-index DataFrame. We only care about the 'Adj Close' price for SPY and the 'Close' price for VIX (as VIX is not a stock and 'Adj Close' is often NaN). We'll select these, rename them for clarity, and create our master DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb89063-73d6-48d0-9679-6e3ee0794f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master DataFrame head:\n",
      "            SPY_Close  VIX_Close\n",
      "Date                            \n",
      "2010-01-04  85.279182  20.040001\n",
      "2010-01-05  85.504944  19.350000\n",
      "2010-01-06  85.565170  19.160000\n",
      "2010-01-07  85.926353  19.059999\n",
      "2010-01-08  86.212273  18.129999\n"
     ]
    }
   ],
   "source": [
    "# Select the relevant columns\n",
    "spy_data = raw_data['Adj Close']['SPY']\n",
    "vix_data = raw_data['Close']['^VIX'] # VIX uses 'Close'\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "master_df = pd.concat([spy_data, vix_data], axis=1)\n",
    "master_df.columns = ['SPY_Close', 'VIX_Close']\n",
    "\n",
    "# Save the raw merged file as a checkpoint\n",
    "master_df.to_csv('SPY_VIX_Raw.csv')\n",
    "\n",
    "print(\"Master DataFrame head:\")\n",
    "print(master_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddacbcdf-4ff4-4668-8c8c-e1f195402a94",
   "metadata": {},
   "source": [
    "## Initial Check (Step 1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8685f8-b71b-4fa9-bce0-b594be849927",
   "metadata": {},
   "source": [
    "We run ```isnull().sum()``` to diagnose gaps. We expect to see missing data, as markets are closed on holidays (e.g., Christmas) and weekends, but ```yfinance``` typically only provides trading-day data. Any NaNs found here are likely from data-source inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "788ac356-c218-44f8-957d-b2799d6eea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before transformation:\n",
      "SPY_Close    0\n",
      "VIX_Close    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Run the initial null check\n",
    "print(\"Missing values before transformation:\")\n",
    "print(master_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ca506-84aa-452e-937e-6f126d389970",
   "metadata": {},
   "source": [
    "# Part 2: Transformation (Data Resilience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5795c-e8f2-4aba-b5d3-93fc270c4357",
   "metadata": {},
   "source": [
    "## NaN Imputation (Step 2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6ec92-0bc9-4f7a-8740-dc45fd589a5c",
   "metadata": {},
   "source": [
    "Financial time-series data must be continuous. A missing value (NaN) implies the market was open but data is missing. We use 'forward-fill' (```ffill```) to fill these gaps. This method assumes the 'last known price' is the most accurate representation for a non-trading day or brief data gap, which is standard practice.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "137e07f8-b9b3-4a80-9b97-3dcf43946b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after ffill:\n",
      "SPY_Close    0\n",
      "VIX_Close    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Forward-fill missing values\n",
    "master_df.ffill(inplace=True)\n",
    "\n",
    "# Run a final check to confirm all gaps are filled\n",
    "print(\"\\nMissing values after ffill:\")\n",
    "print(master_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f0149-7087-4791-ba23-2acdc98a78c9",
   "metadata": {},
   "source": [
    "## Feature Engineering (Trend & Returns) (Step 2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c464311-6a1c-4ea2-8557-a51d14351872",
   "metadata": {},
   "source": [
    "We engineer new features. First, we calculate daily log returns, which are standard in financial modeling. Second, we create a 20-Day Simple Moving Average (SMA_20) on the SPY closing price to model short-term trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c0016fa-0d32-4ffe-b2dd-f88d165a74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily log returns for SPY\n",
    "master_df['SPY_Log_Returns'] = np.log(master_df['SPY_Close'] / master_df['SPY_Close'].shift(1))\n",
    "\n",
    "# Create 20-Day Simple Moving Average (SMA_20)\n",
    "master_df['SMA_20'] = master_df['SPY_Close'].rolling(window=20).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974437b-6c6f-461e-9e6a-78a1e8554064",
   "metadata": {},
   "source": [
    "## Feature Engineering (Risk) (Step 2.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c393a-6586-4f22-9646-4105f56952c6",
   "metadata": {},
   "source": [
    "Price alone is insufficient; we must model risk. We will create a 20-Day rolling volatility, which is the standard deviation of the daily log returns. This feature quantifies market risk over the last 20 trading days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4340b9ba-f0f8-4a97-b356-988bc33cb94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame head after feature engineering:\n",
      "            SPY_Close  VIX_Close  SPY_Log_Returns     SMA_20  Volatility_20\n",
      "Date                                                                       \n",
      "2010-01-04  85.279182  20.040001              NaN        NaN            NaN\n",
      "2010-01-05  85.504944  19.350000         0.002644        NaN            NaN\n",
      "2010-01-06  85.565170  19.160000         0.000704        NaN            NaN\n",
      "2010-01-07  85.926353  19.059999         0.004212        NaN            NaN\n",
      "2010-01-08  86.212273  18.129999         0.003322        NaN            NaN\n",
      "2010-01-11  86.332672  17.549999         0.001396        NaN            NaN\n",
      "2010-01-12  85.527504  18.250000        -0.009370        NaN            NaN\n",
      "2010-01-13  86.249908  17.850000         0.008411        NaN            NaN\n",
      "2010-01-14  86.483192  17.629999         0.002701        NaN            NaN\n",
      "2010-01-15  85.512489  17.910000        -0.011288        NaN            NaN\n",
      "2010-01-19  86.581001  17.580000         0.012418        NaN            NaN\n",
      "2010-01-20  85.700615  18.680000        -0.010220        NaN            NaN\n",
      "2010-01-21  84.052666  22.270000        -0.019416        NaN            NaN\n",
      "2010-01-22  82.178993  27.309999        -0.022544        NaN            NaN\n",
      "2010-01-25  82.600334  25.410000         0.005114        NaN            NaN\n",
      "2010-01-26  82.254219  24.549999        -0.004199        NaN            NaN\n",
      "2010-01-27  82.645515  23.139999         0.004746        NaN            NaN\n",
      "2010-01-28  81.697357  23.730000        -0.011539        NaN            NaN\n",
      "2010-01-29  80.809425  24.620001        -0.010928        NaN            NaN\n",
      "2010-02-01  82.066093  22.590000         0.015431  84.458995            NaN\n",
      "2010-02-02  83.059364  21.480000         0.012031  84.348005       0.010619\n",
      "2010-02-03  82.645515  21.600000        -0.004995  84.205033       0.010606\n",
      "2010-02-04  80.094589  26.080000        -0.031352  83.931504       0.012480\n",
      "2010-02-05  80.260147  26.110001         0.002065  83.648194       0.012421\n",
      "2010-02-08  79.680672  26.510000        -0.007246  83.321614       0.012344\n"
     ]
    }
   ],
   "source": [
    "# Create 20-Day Volatility (std dev of log returns)\n",
    "master_df['Volatility_20'] = master_df['SPY_Log_Returns'].rolling(window=20).std()\n",
    "\n",
    "print(\"\\nDataFrame head after feature engineering:\")\n",
    "print(master_df.head(25)) # Show 25 to see NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605533c-497d-4b08-867c-fe9ff3950b0f",
   "metadata": {},
   "source": [
    "## Clean Start (Step 2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea417d2-c70e-4b9a-aa5a-8b3335259167",
   "metadata": {},
   "source": [
    "Our rolling window calculations (SMA and Volatility) created NaNs for the first 19 rows (since they need 20 days of data). These rows cannot be used for backtesting. We now drop them to create our final, clean, feature-rich dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14f20eb5-c1bb-4a87-bc90-5600868492c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final clean DataFrame head:\n",
      "            SPY_Close  VIX_Close  SPY_Log_Returns     SMA_20  Volatility_20\n",
      "Date                                                                       \n",
      "2010-02-02  83.059364  21.480000         0.012031  84.348005       0.010619\n",
      "2010-02-03  82.645515  21.600000        -0.004995  84.205033       0.010606\n",
      "2010-02-04  80.094589  26.080000        -0.031352  83.931504       0.012480\n",
      "2010-02-05  80.260147  26.110001         0.002065  83.648194       0.012421\n",
      "2010-02-08  79.680672  26.510000        -0.007246  83.321614       0.012344\n",
      "\n",
      "Final null check on clean data:\n",
      "SPY_Close          0\n",
      "VIX_Close          0\n",
      "SPY_Log_Returns    0\n",
      "SMA_20             0\n",
      "Volatility_20      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop the initial NaN rows created by rolling windows\n",
    "clean_df = master_df.dropna()\n",
    "\n",
    "print(\"\\nFinal clean DataFrame head:\")\n",
    "print(clean_df.head())\n",
    "\n",
    "print(\"\\nFinal null check on clean data:\")\n",
    "print(clean_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd0a07-5d10-4bcb-936f-24281307fa59",
   "metadata": {},
   "source": [
    "# Part 3: Load & Documentation (The Deliverable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e2d37-1aaa-45a3-8dd4-a14074da3865",
   "metadata": {},
   "source": [
    "## Load (Step 3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ca92f-8697-4e6d-bc72-1d02ca674047",
   "metadata": {},
   "source": [
    "Finally, we save our clean, feature-rich DataFrame to a new CSV file. This file is now ready for direct input into a backtesting engine (Project 2) or any machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff6efc99-35e0-4292-9d48-cde57a5b1587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'SPY_Clean_Featured_Data.csv' saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the final file\n",
    "clean_df.to_csv('SPY_Clean_Featured_Data.csv')\n",
    "\n",
    "print(\"\\n'SPY_Clean_Featured_Data.csv' saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae083f-d4e8-4eec-9ec3-c3f65cf3dbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
